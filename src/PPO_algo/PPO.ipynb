{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHi7NZghPs9R",
        "outputId": "db721e2a-ae48-455f-fc5f-fa8053bc0c8c"
      },
      "source": [
        "!git clone https://github.com/JoyPang123/RL-Explore-with-Own-made-Env.git\n",
        "!mv RL-Explore-with-Own-made-Env/snake ./snake\n",
        "!pip install -e snake"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'RL-Explore-with-Own-made-Env'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 134 (delta 41), reused 117 (delta 26), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (134/134), 97.49 KiB | 825.00 KiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "Obtaining file:///content/snake\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from snake==0.0.1) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from snake==0.0.1) (1.19.5)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 106 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->snake==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->snake==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->snake==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->snake==0.0.1) (0.16.0)\n",
            "Installing collected packages: pygame, snake\n",
            "  Running setup.py develop for snake\n",
            "Successfully installed pygame-2.1.0 snake-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxzRSUbuP8ri"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7XeQ0GJTOgc"
      },
      "source": [
        "device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDTKZSgMQxCN"
      },
      "source": [
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.done = []\n",
        "    \n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.done[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0V6vWQpPy5R"
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"Adapted from\n",
        "    https://github.com/raillab/a2c/blob/master/a2c/model.py\n",
        "    \"\"\"\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=32,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (32, 32, 32)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 16, 16)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=128,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),  # (2048)\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=32,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (32, 32, 32)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 16, 16)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=128,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),  # (2048)\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        return action.detach(), action_logprob.detach()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, state_values, dist_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njveae_wP4F0"
      },
      "source": [
        "class PPO:\n",
        "    def __init__(self, action_dim, lr_actor, lr_critic, gamma, k_epochs, eps_clip):\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(action_dim).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "        ])\n",
        "\n",
        "        self.policy_old = ActorCritic(action_dim).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "        self.buffer.states.append(state)\n",
        "        self.buffer.actions.append(action)\n",
        "        self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def update(self):\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, done in zip(reversed(self.buffer.rewards), reversed(self.buffer.done)):\n",
        "            if done:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # Converting list to tensor detach for not updating the old policy network\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.k_epochs):\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "\n",
        "            # Final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.criterion(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # Take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # Clear buffer\n",
        "        self.buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipk4OsbySSap",
        "outputId": "f7a6fea1-5dfe-4abe-a9a8-f07b8b54d502"
      },
      "source": [
        "max_ep_len = 1000\n",
        "max_training_timesteps = int(1e5)\n",
        "\n",
        "print_freq = max_ep_len * 4\n",
        "log_freq = max_ep_len * 2\n",
        "\n",
        "update_timestep = max_ep_len * 4\n",
        "k_epochs = 40\n",
        "eps_clip = 0.2\n",
        "gamma = 0.99\n",
        "\n",
        "lr_actor = 3e-4\n",
        "lr_critic = 1e-3\n",
        "\n",
        "env = gym.make(\"snake:snake-v0\")\n",
        "\n",
        "ppo_agent = PPO(env.action_space.n, lr_actor, lr_critic, gamma, k_epochs, eps_clip)\n",
        "\n",
        "running_reward = 0\n",
        "running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "iteration = 0\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64, 64))\n",
        "])\n",
        "\n",
        "while time_step <= max_training_timesteps:\n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len + 1):\n",
        "        # Select action with policy\n",
        "        action = ppo_agent.select_action(img_transforms(state[\"frame\"]).unsqueeze(0))\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Saving the episode information\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.done.append(done)\n",
        "\n",
        "        time_step += 1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        if time_step % print_freq == 0:\n",
        "            avg_reward = running_reward / running_episodes\n",
        "\n",
        "            print(f\"Iteration:{iteration}, get average reward: {avg_reward:.2f}\")\n",
        "\n",
        "            running_reward = 0\n",
        "            running_episodes = 0\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    running_reward += current_ep_reward\n",
        "    running_episodes += 1\n",
        "\n",
        "    iteration += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration:41, get average reward: -8.38\n",
            "Iteration:91, get average reward: -11.63\n",
            "Iteration:138, get average reward: -9.39\n",
            "Iteration:184, get average reward: -9.90\n",
            "Iteration:227, get average reward: -10.07\n",
            "Iteration:262, get average reward: -7.86\n",
            "Iteration:288, get average reward: -11.77\n",
            "Iteration:318, get average reward: -9.68\n",
            "Iteration:347, get average reward: -9.34\n",
            "Iteration:370, get average reward: -6.07\n",
            "Iteration:389, get average reward: -9.37\n",
            "Iteration:407, get average reward: -7.33\n",
            "Iteration:430, get average reward: -10.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnC-xRqaTCOH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}