{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_Discrete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLvVOMLQbslb"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Pb6OJUerSj"
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import gym\n",
        "\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOpxApNSjLHn"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=32,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (32, 32, 32)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 16, 16)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=128,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),  # (2048)\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.actor(x)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, act_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=32,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (32, 32, 32)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 16, 16)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=128,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),  # (2048)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4 + act_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = self.critic(state)\n",
        "        x = torch.cat([x, action], dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7NxKPU-enKd"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_len):\n",
        "        self.replay = deque(maxlen=max_len)\n",
        "\n",
        "    def store_experience(self, state, reward,\n",
        "                         action, next_state,\n",
        "                         done):\n",
        "        self.replay.append([state, reward, action, next_state, done])\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.replay)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.replay) < batch_size:\n",
        "            return None\n",
        "\n",
        "        return random.sample(self.replay, k=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0i_hil2hdE8"
      },
      "source": [
        "class DDPG:\n",
        "    def __init__(self, memory_size, num_actions,\n",
        "                 actor_lr, critic_lr, gamma,\n",
        "                 tau, device, img_transforms):\n",
        "        # Set up model\n",
        "        self.actor = Actor(num_actions).to(device)\n",
        "        self.target_actor = Actor(num_actions).to(device)\n",
        "        self.target_actor.eval()\n",
        "        self.critic = Critic(num_actions).to(device)\n",
        "        self.target_critic = Critic(num_actions).to(device)\n",
        "        self.target_critic.eval()\n",
        "\n",
        "        # Set up optimizer and criterion\n",
        "        self.critic_criterion = nn.MSELoss()\n",
        "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "\n",
        "        # Set up transforms and other hyper-parameters\n",
        "        self.device = device\n",
        "        self.img_transforms = img_transforms\n",
        "        self.num_actions = num_actions\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "    def choose_action(self, cur_state, eps):\n",
        "        # Open evaluation mode\n",
        "        self.actor.eval()\n",
        "\n",
        "        # Exploration\n",
        "        if np.random.uniform() < eps:\n",
        "            action = np.random.randint(0, self.num_actions)\n",
        "        else:  # Exploitation\n",
        "            cur_state = self.img_transforms(cur_state).to(self.device).unsqueeze(0)\n",
        "            action_list = self.actor(cur_state)\n",
        "            action = torch.argmax(action_list, dim=-1).item()\n",
        "\n",
        "        # Open training mode\n",
        "        self.actor.train()\n",
        "        return action\n",
        "\n",
        "    def actor_update(self, batch_data):\n",
        "        # Separate the data into groups\n",
        "        cur_state_batch = []\n",
        "\n",
        "        for cur_state, *_ in batch_data:\n",
        "            cur_state_batch.append(self.img_transforms(cur_state).unsqueeze(0))\n",
        "\n",
        "        cur_state_batch = torch.cat(cur_state_batch, dim=0).to(self.device)\n",
        "        actor_actions = F.gumbel_softmax(torch.log(F.softmax(self.actor(cur_state_batch), dim=1)), hard=True)\n",
        "\n",
        "        loss = -self.critic(cur_state_batch, actor_actions).mean()\n",
        "        self.actor_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "    def critic_update(self, batch_data):\n",
        "        # Separate the data into groups\n",
        "        cur_state_batch = []\n",
        "        reward_batch = []\n",
        "        action_batch = []\n",
        "        next_state_batch = []\n",
        "        done_batch = []\n",
        "\n",
        "        for cur_state, reward, action, next_state, done in batch_data:\n",
        "            cur_state_batch.append(self.img_transforms(cur_state).unsqueeze(0))\n",
        "            reward_batch.append(reward)\n",
        "            action_batch.append(action)\n",
        "            next_state_batch.append(self.img_transforms(next_state).unsqueeze(0))\n",
        "            done_batch.append(done)\n",
        "\n",
        "        cur_state_batch = torch.cat(cur_state_batch, dim=0).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
        "        action_batch = torch.LongTensor(action_batch)\n",
        "        action_batch = torch.zeros(len(batch_data), self.num_actions).scatter_(\n",
        "            1, action_batch.unsqueeze(1), 1).to(self.device)\n",
        "        next_state_batch = torch.cat(next_state_batch, dim=0).to(self.device)\n",
        "        done_batch = torch.Tensor(done_batch).to(self.device)\n",
        "\n",
        "        # Compute the TD error between eval and target\n",
        "        Q_eval = self.critic(cur_state_batch, action_batch)\n",
        "        next_action = F.softmax(self.target_actor(next_state_batch), dim=1)\n",
        "\n",
        "        index = torch.argmax(next_action, dim=1).unsqueeze(1)\n",
        "        next_action = torch.zeros_like(next_action).scatter_(1, index, 1).to(self.device)\n",
        "        Q_target = reward_batch + self.gamma * (1 - done_batch) * self.target_critic(next_state_batch,\n",
        "                                                                                     next_action).squeeze(1)\n",
        "\n",
        "        loss = self.critic_criterion(Q_eval.squeeze(1), Q_target)\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "    def soft_update(self):\n",
        "        # EMA for both actor and critic network\n",
        "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm5jwBzTh0Jo"
      },
      "source": [
        "env = gym.make(\"snake:snake-v0\", mode=\"hardworking\")\n",
        "device = \"cpu\"\n",
        "\n",
        "# Set up environment hyperparameters\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Set up training hyperparameters\n",
        "tau = 0.05\n",
        "max_time_steps = 100000\n",
        "max_iter = 2000\n",
        "gamma = 0.9\n",
        "memory_size = 2000\n",
        "batch_size = 32\n",
        "actor_lr = 3e-4\n",
        "critic_lr = 3e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV0L7xy4nuw1"
      },
      "source": [
        "def train(max_time_steps, max_iter, memory_size, \n",
        "          num_actions, actor_lr, critic_lr,\n",
        "          gamma, tau, device, batch_size):\n",
        "    \n",
        "    # Set up model training\n",
        "    img_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((64, 64))\n",
        "    ])\n",
        "\n",
        "    ddpg = DDPG(\n",
        "        memory_size, num_actions, \n",
        "        actor_lr, critic_lr, gamma,\n",
        "        tau, device, img_transforms\n",
        "    )\n",
        "    max_reward = -1e-9\n",
        "\n",
        "    running_reward = 0\n",
        "    running_episodes = 0\n",
        "\n",
        "    time_step = 0\n",
        "    print_freq = max_iter * 2\n",
        "\n",
        "    while time_step < max_time_steps:\n",
        "        state = env.reset()\n",
        "        current_ep_reward = 0\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            # Get reward and state\n",
        "            actions = ddpg.choose_action(state[\"frame\"], 0.1)\n",
        "            new_state, reward, done, _ = env.step(actions)\n",
        "\n",
        "            current_ep_reward += reward\n",
        "            ddpg.memory.store_experience(state[\"frame\"], reward, actions, new_state[\"frame\"], done)\n",
        "            state = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # Wait for updating\n",
        "            if ddpg.memory.size() < batch_size:\n",
        "                continue\n",
        "\n",
        "            batch_data = ddpg.memory.sample(batch_size)\n",
        "            ddpg.critic_update(batch_data)\n",
        "            ddpg.actor_update(batch_data)\n",
        "            ddpg.soft_update()\n",
        "\n",
        "            time_step += 1\n",
        "\n",
        "            if time_step % print_freq == 0:\n",
        "                avg_reward = running_reward / running_episodes\n",
        "\n",
        "                print(f\"Iteration:{running_episodes}, get average reward: {avg_reward:.2f}\")\n",
        "\n",
        "                running_reward = 0\n",
        "                running_episodes = 0\n",
        "                log = {\n",
        "                    \"avg_reward\": avg_reward,\n",
        "                }\n",
        "                wandb.log(log)\n",
        "\n",
        "                if avg_reward > max_reward:\n",
        "                    max_reward = avg_reward\n",
        "                    torch.save(ddpg.actor.state_dict(), \"actor_best.pt\")\n",
        "                    torch.save(ddpg.critic.state_dict(), \"critic_best.pt\")\n",
        "        \n",
        "        running_reward += current_ep_reward\n",
        "        running_episodes += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGoYRlmSaCLJ"
      },
      "source": [
        "model_config = {\n",
        "    \"gamma\": gamma,\n",
        "    \"max_time_steps\": max_time_steps,\n",
        "    \"memory size\": memory_size\n",
        "}\n",
        "run = wandb.init(\n",
        "    project=\"snake_RL\",\n",
        "    resume=False,\n",
        "    config=model_config,\n",
        "    name=\"DDPG\"\n",
        ")\n",
        "\n",
        "train(\n",
        "    max_time_steps, max_iter, memory_size, \n",
        "    4, actor_lr, critic_lr,\n",
        "    gamma, tau, \"cpu\", batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfpjpOLZaE_W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}