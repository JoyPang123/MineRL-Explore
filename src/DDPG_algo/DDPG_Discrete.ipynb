{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_Discrete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9AUFL16jFK5",
        "outputId": "2a559ee6-3bc3-4f4a-e1a5-2788ad92d11a"
      },
      "source": [
        "!git clone https://github.com/JoyPang123/snake_env.git\n",
        "!mv snake_env/snake ./snake\n",
        "!pip install -e snake\n",
        "exit() # Leave it here for automatically restart the runtime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'snake_env'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 194 (delta 70), reused 160 (delta 38), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (194/194), 163.41 KiB | 3.27 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n",
            "Obtaining file:///content/snake\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from snake==0.0.1) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from snake==0.0.1) (1.19.5)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 85 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->snake==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->snake==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->snake==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->snake==0.0.1) (0.16.0)\n",
            "Installing collected packages: pygame, snake\n",
            "  Running setup.py develop for snake\n",
            "Successfully installed pygame-2.1.0 snake-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLvVOMLQbslb",
        "outputId": "cb35cf39-0a80-4a54-cadc-60e0aaa738e5"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 34.5 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=d99219831013bed84ddddc5463298ad25c5f5e8f5fc694fcbc0b26c286f85c67\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=df4d7dc4aaf7ac26156caf0d6e3da0c55c07523ed15a8041d669a7a6c4d72752\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Pb6OJUerSj"
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import gym\n",
        "\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOpxApNSjLHn"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=32,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (32, 32, 32)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 16, 16)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=128,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),  # (2048)\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.actor(x)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, act_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=32,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (32, 32, 32)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 16, 16)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=64,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, out_channels=128,\n",
        "                kernel_size=3, padding=1, stride=2\n",
        "            ),  # (128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),  # (2048)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4 + act_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = self.critic(state)\n",
        "        x = torch.cat([x, action], dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7NxKPU-enKd"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_len):\n",
        "        self.replay = deque(maxlen=max_len)\n",
        "\n",
        "    def store_experience(self, state, reward,\n",
        "                         action, next_state,\n",
        "                         done):\n",
        "        self.replay.append([state, reward, action, next_state, done])\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.replay)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.replay) < batch_size:\n",
        "            return None\n",
        "\n",
        "        return random.sample(self.replay, k=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0i_hil2hdE8"
      },
      "source": [
        "class DDPG:\n",
        "    def __init__(self, memory_size, num_actions,\n",
        "                 actor_lr, critic_lr, gamma,\n",
        "                 tau, device, img_transforms):\n",
        "        # Set up model\n",
        "        self.actor = Actor(num_actions).to(device)\n",
        "        self.target_actor = Actor(num_actions).to(device)\n",
        "        self.target_actor.eval()\n",
        "        self.critic = Critic(num_actions).to(device)\n",
        "        self.target_critic = Critic(num_actions).to(device)\n",
        "        self.target_critic.eval()\n",
        "\n",
        "        # Set up optimizer and criterion\n",
        "        self.critic_criterion = nn.MSELoss()\n",
        "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "\n",
        "        # Set up transforms and other hyper-parameters\n",
        "        self.device = device\n",
        "        self.img_transforms = img_transforms\n",
        "        self.num_actions = num_actions\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "    def choose_action(self, cur_state, eps):\n",
        "        # Open evaluation mode\n",
        "        self.actor.eval()\n",
        "\n",
        "        # Exploration\n",
        "        if np.random.uniform() < eps:\n",
        "            action = np.random.randint(0, self.num_actions)\n",
        "        else:  # Exploitation\n",
        "            cur_state = self.img_transforms(cur_state).to(self.device).unsqueeze(0)\n",
        "            action_list = self.actor(cur_state)\n",
        "            action = torch.argmax(action_list, dim=-1).item()\n",
        "\n",
        "        # Open training mode\n",
        "        self.actor.train()\n",
        "        return action\n",
        "\n",
        "    def actor_update(self, batch_data):\n",
        "        # Separate the data into groups\n",
        "        cur_state_batch = []\n",
        "\n",
        "        for cur_state, *_ in batch_data:\n",
        "            cur_state_batch.append(self.img_transforms(cur_state).unsqueeze(0))\n",
        "\n",
        "        cur_state_batch = torch.cat(cur_state_batch, dim=0).to(self.device)\n",
        "        actor_actions = F.gumbel_softmax(torch.log(F.softmax(self.actor(cur_state_batch), dim=1)), hard=True)\n",
        "\n",
        "        loss = -self.critic(cur_state_batch, actor_actions).mean()\n",
        "        self.actor_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "    def critic_update(self, batch_data):\n",
        "        # Separate the data into groups\n",
        "        cur_state_batch = []\n",
        "        reward_batch = []\n",
        "        action_batch = []\n",
        "        next_state_batch = []\n",
        "        done_batch = []\n",
        "\n",
        "        for cur_state, reward, action, next_state, done in batch_data:\n",
        "            cur_state_batch.append(self.img_transforms(cur_state).unsqueeze(0))\n",
        "            reward_batch.append(reward)\n",
        "            action_batch.append(action)\n",
        "            next_state_batch.append(self.img_transforms(next_state).unsqueeze(0))\n",
        "            done_batch.append(done)\n",
        "\n",
        "        cur_state_batch = torch.cat(cur_state_batch, dim=0).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
        "        action_batch = torch.LongTensor(action_batch)\n",
        "        action_batch = torch.zeros(len(batch_data), self.num_actions).scatter_(\n",
        "            1, action_batch.unsqueeze(1), 1).to(self.device)\n",
        "        next_state_batch = torch.cat(next_state_batch, dim=0).to(self.device)\n",
        "        done_batch = torch.Tensor(done_batch).to(self.device)\n",
        "\n",
        "        # Compute the TD error between eval and target\n",
        "        Q_eval = self.critic(cur_state_batch, action_batch)\n",
        "        next_action = F.softmax(self.target_actor(next_state_batch), dim=1)\n",
        "\n",
        "        index = torch.argmax(next_action, dim=1).unsqueeze(1)\n",
        "        next_action = torch.zeros_like(next_action).scatter_(1, index, 1).to(self.device)\n",
        "        Q_target = reward_batch + self.gamma * (1 - done_batch) * self.target_critic(next_state_batch,\n",
        "                                                                                     next_action).squeeze(1)\n",
        "\n",
        "        loss = self.critic_criterion(Q_eval.squeeze(1), Q_target)\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "    def soft_update(self):\n",
        "        # EMA for both actor and critic network\n",
        "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm5jwBzTh0Jo"
      },
      "source": [
        "env = gym.make(\"snake:snake-v0\", mode=\"hardworking\")\n",
        "device = \"cpu\"\n",
        "\n",
        "# Set up environment hyperparameters\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Set up training hyperparameters\n",
        "tau = 0.05\n",
        "max_time_steps = 100000\n",
        "max_iter = 2000\n",
        "gamma = 0.9\n",
        "memory_size = 2000\n",
        "batch_size = 32\n",
        "actor_lr = 3e-4\n",
        "critic_lr = 3e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV0L7xy4nuw1"
      },
      "source": [
        "def train(max_time_steps, max_iter, memory_size, \n",
        "          num_actions, actor_lr, critic_lr,\n",
        "          gamma, tau, device, batch_size):\n",
        "    \n",
        "    # Set up model training\n",
        "    img_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((64, 64))\n",
        "    ])\n",
        "\n",
        "    ddpg = DDPG(\n",
        "        memory_size, num_actions, \n",
        "        actor_lr, critic_lr, gamma,\n",
        "        tau, device, img_transforms\n",
        "    )\n",
        "    max_reward = -1e-9\n",
        "\n",
        "    running_reward = 0\n",
        "    running_episodes = 0\n",
        "\n",
        "    time_step = 0\n",
        "    print_freq = max_iter * 2\n",
        "\n",
        "    while time_step < max_time_steps:\n",
        "        state = env.reset()\n",
        "        current_ep_reward = 0\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            # Get reward and state\n",
        "            actions = ddpg.choose_action(state[\"frame\"], 0.1)\n",
        "            new_state, reward, done, _ = env.step(actions)\n",
        "\n",
        "            current_ep_reward += reward\n",
        "            ddpg.memory.store_experience(state[\"frame\"], reward, actions, new_state[\"frame\"], done)\n",
        "            state = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # Wait for updating\n",
        "            if ddpg.memory.size() < batch_size:\n",
        "                continue\n",
        "\n",
        "            batch_data = ddpg.memory.sample(batch_size)\n",
        "            ddpg.critic_update(batch_data)\n",
        "            ddpg.actor_update(batch_data)\n",
        "            ddpg.soft_update()\n",
        "\n",
        "            time_step += 1\n",
        "\n",
        "            if time_step % print_freq == 0:\n",
        "                avg_reward = running_reward / running_episodes\n",
        "\n",
        "                print(f\"Iteration:{running_episodes}, get average reward: {avg_reward:.2f}\")\n",
        "\n",
        "                running_reward = 0\n",
        "                running_episodes = 0\n",
        "                log = {\n",
        "                    \"avg_reward\": avg_reward,\n",
        "                }\n",
        "                wandb.log(log)\n",
        "\n",
        "                if avg_reward > max_reward:\n",
        "                    max_reward = avg_reward\n",
        "                    torch.save(ddpg.actor.state_dict(), \"actor_best.pt\")\n",
        "                    torch.save(ddpg.critic.state_dict(), \"critic_best.pt\")\n",
        "        \n",
        "        running_reward += current_ep_reward\n",
        "        running_episodes += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "cGoYRlmSaCLJ",
        "outputId": "ecb6fe80-a64e-4b71-cd35-2dad36cbb6e5"
      },
      "source": [
        "model_config = {\n",
        "    \"gamma\": gamma,\n",
        "    \"max_time_steps\": max_time_steps,\n",
        "    \"memory size\": memory_size\n",
        "}\n",
        "run = wandb.init(\n",
        "    project=\"snake_RL\",\n",
        "    resume=False,\n",
        "    config=model_config,\n",
        "    name=\"DDPG\"\n",
        ")\n",
        "\n",
        "train(\n",
        "    max_time_steps, max_iter, memory_size, \n",
        "    4, actor_lr, critic_lr,\n",
        "    gamma, tau, \"cpu\", batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoypang\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/joypang/snake_RL/runs/19iux7op\" target=\"_blank\">DDPG</a></strong> to <a href=\"https://wandb.ai/joypang/snake_RL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:746, get average reward: -9.36\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-65365e167c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-25-bdc2b0e1d9ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_time_steps, max_iter, memory_size, num_actions, actor_lr, critic_lr, gamma, tau, device, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-9f4cb39df124>\u001b[0m in \u001b[0;36mcritic_update\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mreward_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0maction_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mdone_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfpjpOLZaE_W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}