{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "erKr9mtFBatK"
      },
      "source": [
        "!git clone https://github.com/JoyPang123/snake_env.git\n",
        "!mv snake_env/snake ./snake\n",
        "!pip install -e snake\n",
        "exit() # Leave it here for automatically restart the runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "9KK0EDK3Waja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnQholMMB0bq"
      },
      "source": [
        "import math\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqEbAYxbSCKa"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_actions, in_channels=3):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (16, 16, 16)\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=8,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (8, 8, 8)\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(64 * 8, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUX8z_yxT0rF"
      },
      "source": [
        "class DQN():\n",
        "    def __init__(self, num_actions, device,\n",
        "                 replace_iter=150, max_len=100,\n",
        "                 EPS_START=0.9, EPS_END=0.05, EPS_DECAY=200):\n",
        "        # Create network for target and evaluation\n",
        "        self.eval_net = Model(num_actions=num_actions).to(device)\n",
        "        self.target_net = Model(num_actions=num_actions).to(device)\n",
        "        \n",
        "        # Set up the replay experience\n",
        "        self.replay = deque(maxlen=max_len)\n",
        "\n",
        "        # Transform the image\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(\n",
        "                (32, 32), interpolation=InterpolationMode.BICUBIC\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Set up the counter to update target from eval\n",
        "        self.target_counter = 0\n",
        "\n",
        "        # Set up hyperparameters\n",
        "        self.device = device\n",
        "        self.num_actions = num_actions\n",
        "        self.replace_iter = replace_iter\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # For exploration probability \n",
        "        self.EPS_START = EPS_START\n",
        "        self.EPS_END = EPS_END\n",
        "        self.EPS_DECAY = EPS_DECAY\n",
        "        self.step_total_count = 0\n",
        "\n",
        "    def choose_action(self, cur_state):\n",
        "        # Open evaluation mode\n",
        "        self.eval_net.eval()\n",
        "\n",
        "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) \\\n",
        "            * math.exp(-1. * self.step_total_count / self.EPS_DECAY)\n",
        "        self.step_total_count += 1\n",
        "\n",
        "        \"\"\"Choose the action using epsilon greedy policy\"\"\"\n",
        "        # Exploration\n",
        "        if np.random.uniform() < eps_threshold:\n",
        "            action = np.random.randint(0, self.num_actions)\n",
        "        else: # Exploitation\n",
        "            cur_state = self.transforms(cur_state).to(device).unsqueeze(0)\n",
        "            action_list = self.eval_net(cur_state)\n",
        "            action = torch.argmax(action_list, dim=-1).item()\n",
        "\n",
        "        # Open training mode\n",
        "        self.eval_net.train()\n",
        "        return action\n",
        "\n",
        "    def store_experience(self, state, reward,\n",
        "                         action, next_state,\n",
        "                         done):\n",
        "        \"\"\"Record the play experience into deque\n",
        "\n",
        "        The format of the experience:\n",
        "            [state, reward, action, next_state, done]\n",
        "        \"\"\"\n",
        "\n",
        "        self.replay.append([state, reward, action, next_state, done])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7LvXIiDVFB-"
      },
      "source": [
        "def update(model, batch_size,\n",
        "           optimizer, criterion,\n",
        "           tau=0.3,\n",
        "           gamma=0.9):\n",
        "    # Set up the device same as model\n",
        "    used_device = model.device\n",
        "    # Get the data from the experience\n",
        "    batch_data = random.sample(model.replay,\n",
        "                               batch_size)\n",
        "    \n",
        "    # Seperate the data into groups\n",
        "    cur_state_batch = []\n",
        "    reward_batch = []\n",
        "    action_batch = []\n",
        "    next_state_batch = []\n",
        "    done_batch = []\n",
        "\n",
        "    for cur_state, reward, action, next_state, done in batch_data:\n",
        "        cur_state_batch.append(model.transforms(cur_state).unsqueeze(0))\n",
        "        reward_batch.append(reward)\n",
        "        action_batch.append(action)\n",
        "        next_state_batch.append(model.transforms(next_state).unsqueeze(0))\n",
        "        done_batch.append(done)\n",
        "\n",
        "    cur_state_batch = torch.cat(cur_state_batch, dim=0).to(device)\n",
        "    reward_batch = torch.FloatTensor(reward_batch).to(device)\n",
        "    action_batch = torch.FloatTensor(action_batch).to(device)\n",
        "    next_state_batch = torch.cat(next_state_batch, dim=0).to(device)\n",
        "    done_batch = torch.Tensor(done_batch).to(device)\n",
        "\n",
        "    # Compute the error between eval and target net\n",
        "    Q_eval = model.eval_net(cur_state_batch).gather(\n",
        "        dim=1,\n",
        "        index=action_batch.long().unsqueeze(1)\n",
        "    ).squeeze(1)\n",
        "\n",
        "    # Detach from target net to avoid computing the gradient\n",
        "    Q_next = model.target_net(next_state_batch).detach()\n",
        "    Q_target = reward_batch + gamma * (1 - done_batch) * torch.max(Q_next, dim=1)[0]\n",
        "\n",
        "    # Compute loss and update the model\n",
        "    loss = criterion(Q_eval, Q_target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the counter for the eval\n",
        "    model.step_counter += 1\n",
        "    \n",
        "    # Replace target net with eval net\n",
        "    if model.step_counter == model.replace_iter:\n",
        "        model.step_counter = 0\n",
        "        for eval_parameters, target_parameters in zip(model.eval_net.parameters(),\n",
        "                                                      model.target_net.parameters()):\n",
        "            target_parameters.data.copy_(tau * eval_parameters.data + \\\n",
        "                                         (1.0 - tau) * target_parameters.data)\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-hditngVuLf"
      },
      "source": [
        "def train(model, env, optimizer,\n",
        "          criterion, iteration,\n",
        "          batch_size,\n",
        "          tau=0.3,\n",
        "          gamma=0.9):\n",
        "    # Save the training info\n",
        "    average_reward_history = []\n",
        "    reward_history = []\n",
        "    loss_history = []\n",
        "    total_rewards = 0\n",
        "\n",
        "    record_reward_time_step = 0\n",
        "    running_reward = 0\n",
        "    record_episode = 0\n",
        "    print_freq = 400\n",
        "    max_reward = -1e9\n",
        "\n",
        "    for cur_iter in range(iteration):\n",
        "        print(f\"===========Iteration {cur_iter + 1}/{iteration}============\")\n",
        "        time_step = 0\n",
        "        rewards = 0\n",
        "        state = env.reset()[\"frame\"]\n",
        "\n",
        "        while True:\n",
        "            # Choose action\n",
        "            action = model.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Store experience\n",
        "            model.store_experience(state, reward, action, next_state[\"frame\"], done)\n",
        "\n",
        "            # Add rewards\n",
        "            rewards += reward\n",
        "            running_reward += reward\n",
        "\n",
        "            # Train if the experience is enough\n",
        "            if len(model.replay) > batch_size:\n",
        "                loss = update(model=model,\n",
        "                              batch_size=batch_size,\n",
        "                              optimizer=optimizer,\n",
        "                              criterion=criterion,\n",
        "                              tau=tau,\n",
        "                              gamma=gamma)\n",
        "                loss_history.append(loss)\n",
        "\n",
        "            # Step into next state\n",
        "            state = next_state[\"frame\"]\n",
        "\n",
        "            # Check whether current model is done or not\n",
        "            if done:\n",
        "                print(f\"Iteration finished after {time_step + 1} timesteps\")\n",
        "                print(f\"Get total rewards {rewards}\")\n",
        "                print(f\"The length of the snake is {env.snake.length}\")\n",
        "                break\n",
        "\n",
        "            time_step += 1\n",
        "            record_reward_time_step += 1\n",
        "\n",
        "            if record_reward_time_step % print_freq == 0:\n",
        "                avg_reward = running_reward / (record_episode + 1) \n",
        "\n",
        "                running_reward = 0\n",
        "                record_episode = 0\n",
        "                log = {\n",
        "                    \"avg_reward\": avg_reward,\n",
        "                }\n",
        "                wandb.log(log)\n",
        "\n",
        "                if avg_reward > max_reward:\n",
        "                    max_reward = avg_reward\n",
        "                    torch.save(model.eval_net.state_dict(), \"best.pt\")\n",
        "\n",
        "        reward_history.append(rewards)\n",
        "        total_rewards += rewards\n",
        "        average_reward_history.append(total_rewards / (cur_iter + 1))\n",
        "        record_episode += 1\n",
        "\n",
        "    return average_reward_history, reward_history, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0plpCks1WBXK"
      },
      "source": [
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Used: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlZWwrYuWHMg"
      },
      "source": [
        "env = gym.make(\"snake:snake-v0\")\n",
        "\n",
        "# Set up environment hyperparameters\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Set up training hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "tau = 1.0\n",
        "replace_iter = 10\n",
        "max_len = 1000\n",
        "iteration = 4000\n",
        "\n",
        "# Used for the exploration\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "\n",
        "# Build the model\n",
        "dqn_model = DQN(\n",
        "    num_actions=num_actions, \n",
        "    device=device, replace_iter=replace_iter,\n",
        "    max_len=max_len, EPS_START=EPS_START, EPS_END=EPS_END,\n",
        "    EPS_DECAY=EPS_DECAY\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7WOsNdkWb6N"
      },
      "source": [
        "# Set up optimizer and criterion\n",
        "optimizer = optim.Adam(dqn_model.eval_net.parameters(),\n",
        "                        lr=learning_rate)\n",
        "criterion = nn.SmoothL1Loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"gamma\": gamma,\n",
        "    \"mode\": \"hardworking\"\n",
        "}\n",
        "run = wandb.init(\n",
        "    project=\"snake_RL\",\n",
        "    resume=False,\n",
        "    config=model_config,\n",
        "    name=\"DQN\"\n",
        ")"
      ],
      "metadata": {
        "id": "VLdBN8TUWyv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTRKhL2XUsK"
      },
      "source": [
        "# Start training DQN\n",
        "average_reward_history, reward_history, \\\n",
        "    loss_history = train(\n",
        "        model=dqn_model, env=env, optimizer=optimizer,\n",
        "        criterion=criterion, iteration=iteration,\n",
        "        batch_size=batch_size, tau=tau, gamma=gamma\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ezv80qPXhT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}