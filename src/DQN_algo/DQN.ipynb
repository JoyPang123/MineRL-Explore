{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "erKr9mtFBatK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3c97cf-0609-4676-a73e-af0e78fe2ff9"
      },
      "source": [
        "!git clone https://github.com/JoyPang123/RL-Explore-with-Own-made-Env.git\n",
        "!mv RL-Explore-with-Own-made-Env/snake ./snake\n",
        "!pip install -e snake"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RL-Explore-with-Own-made-Env'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 55 (delta 16), reused 48 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnQholMMB0bq"
      },
      "source": [
        "import math\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqEbAYxbSCKa"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_actions, in_channels=3):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Create the layers for the model\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (16, 16, 16)\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=8,\n",
        "                kernel_size=5, padding=2, stride=2\n",
        "            ),  # (8, 8, 8)\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(64 * 8, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUX8z_yxT0rF"
      },
      "source": [
        "class DQN():\n",
        "    def __init__(self, num_actions, device,\n",
        "                 replace_iter=150, max_len=100,\n",
        "                 EPS_START=0.9, EPS_END=0.05, EPS_DECAY=200):\n",
        "        # Create network for target and evaluation\n",
        "        self.eval_net = Model(num_actions=num_actions).to(device)\n",
        "        self.target_net = Model(num_actions=num_actions).to(device)\n",
        "        \n",
        "        # Set up the replay experience\n",
        "        self.replay = deque(maxlen=max_len)\n",
        "\n",
        "        # Transform the image\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(\n",
        "                (32, 32), interpolation=InterpolationMode.BICUBIC\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Set up the counter to update target from eval\n",
        "        self.target_counter = 0\n",
        "\n",
        "        # Set up hyperparameters\n",
        "        self.device = device\n",
        "        self.num_actions = num_actions\n",
        "        self.replace_iter = replace_iter\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # For exploration probability \n",
        "        self.EPS_START = EPS_START\n",
        "        self.EPS_END = EPS_END\n",
        "        self.EPS_DECAY = EPS_DECAY\n",
        "        self.step_total_count = 0\n",
        "\n",
        "    def choose_action(self, cur_state):\n",
        "        # Open evaluation mode\n",
        "        self.eval_net.eval()\n",
        "\n",
        "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) \\\n",
        "            * math.exp(-1. * self.step_total_count / self.EPS_DECAY)\n",
        "        self.step_total_count += 1\n",
        "\n",
        "        \"\"\"Choose the action using epsilon greedy policy\"\"\"\n",
        "        # Exploration\n",
        "        if np.random.uniform() < eps_threshold:\n",
        "            action = np.random.randint(0, self.num_actions)\n",
        "        else: # Exploitation\n",
        "            cur_state = self.transforms(cur_state).to(device).unsqueeze(0)\n",
        "            action_list = self.eval_net(cur_state)\n",
        "            action = torch.argmax(action_list, dim=-1).item()\n",
        "\n",
        "        # Open training mode\n",
        "        self.eval_net.train()\n",
        "        return action\n",
        "\n",
        "    def store_experience(self, state, reward,\n",
        "                         action, next_state,\n",
        "                         done):\n",
        "        \"\"\"Record the play experience into deque\n",
        "\n",
        "        The format of the experience:\n",
        "            [state, reward, action, next_state, done]\n",
        "        \"\"\"\n",
        "\n",
        "        self.replay.append([state, reward, action, next_state, done])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7LvXIiDVFB-"
      },
      "source": [
        "def update(model, batch_size,\n",
        "           optimizer, criterion,\n",
        "           tau=0.3,\n",
        "           gamma=0.9):\n",
        "    # Set up the device same as model\n",
        "    used_device = model.device\n",
        "    # Get the data from the experience\n",
        "    batch_data = random.sample(model.replay,\n",
        "                               batch_size)\n",
        "    \n",
        "    # Seperate the data into groups\n",
        "    cur_state_batch = []\n",
        "    reward_batch = []\n",
        "    action_batch = []\n",
        "    next_state_batch = []\n",
        "    done_batch = []\n",
        "\n",
        "    for cur_state, reward, action, next_state, done in batch_data:\n",
        "        cur_state_batch.append(model.transforms(cur_state).unsqueeze(0))\n",
        "        reward_batch.append(reward)\n",
        "        action_batch.append(action)\n",
        "        next_state_batch.append(model.transforms(next_state).unsqueeze(0))\n",
        "        done_batch.append(done)\n",
        "\n",
        "    cur_state_batch = torch.cat(cur_state_batch, dim=0).to(device)\n",
        "    reward_batch = torch.FloatTensor(reward_batch).to(device)\n",
        "    action_batch = torch.FloatTensor(action_batch).to(device)\n",
        "    next_state_batch = torch.cat(next_state_batch, dim=0).to(device)\n",
        "    done_batch = torch.Tensor(done_batch).to(device)\n",
        "\n",
        "    # Compute the error between eval and target net\n",
        "    Q_eval = model.eval_net(cur_state_batch).gather(\n",
        "        dim=1,\n",
        "        index=action_batch.long().unsqueeze(1)\n",
        "    ).squeeze(1)\n",
        "\n",
        "    # Detach from target net to avoid computing the gradient\n",
        "    Q_next = model.target_net(next_state_batch).detach()\n",
        "    Q_target = reward_batch + gamma * (1 - done_batch) * torch.max(Q_next, dim=1)[0]\n",
        "\n",
        "    # Compute loss and update the model\n",
        "    loss = criterion(Q_eval, Q_target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the counter for the eval\n",
        "    model.step_counter += 1\n",
        "    \n",
        "    # Replace target net with eval net\n",
        "    if model.step_counter == model.replace_iter:\n",
        "        model.step_counter = 0\n",
        "        for eval_parameters, target_parameters in zip(model.eval_net.parameters(),\n",
        "                                                      model.target_net.parameters()):\n",
        "            target_parameters.data.copy_(tau * eval_parameters.data + \\\n",
        "                                         (1.0 - tau) * target_parameters.data)\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-hditngVuLf"
      },
      "source": [
        "def train(model, env, optimizer,\n",
        "          criterion, iteration,\n",
        "          batch_size,\n",
        "          tau=0.3,\n",
        "          gamma=0.9):\n",
        "    # Save the training info\n",
        "    average_reward_history = []\n",
        "    reward_history = []\n",
        "    loss_history = []\n",
        "    total_rewards = 0\n",
        "\n",
        "    for cur_iter in range(iteration):\n",
        "        print(f\"===========Iteration {cur_iter + 1}/{iteration}============\")\n",
        "        time_step = 0\n",
        "        rewards = 0\n",
        "        state = env.reset()[\"frame\"]\n",
        "\n",
        "        while True:\n",
        "            # Choose action\n",
        "            action = model.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Store experience\n",
        "            model.store_experience(state, reward, action, next_state[\"frame\"], done)\n",
        "\n",
        "            # Add rewards\n",
        "            rewards += reward\n",
        "\n",
        "            # Train if the experience is enough\n",
        "            if len(model.replay) > batch_size:\n",
        "                loss = update(model=model,\n",
        "                              batch_size=batch_size,\n",
        "                              optimizer=optimizer,\n",
        "                              criterion=criterion,\n",
        "                              tau=tau,\n",
        "                              gamma=gamma)\n",
        "                loss_history.append(loss)\n",
        "\n",
        "            # Step into next state\n",
        "            state = next_state[\"frame\"]\n",
        "\n",
        "            # Check whether current model is done or not\n",
        "            if done:\n",
        "                print(f\"Iteration finished after {time_step + 1} timesteps\")\n",
        "                print(f\"Get total rewards {rewards}\")\n",
        "                print(f\"The length of the snake is {env.snake.length}\")\n",
        "                break\n",
        "\n",
        "            time_step += 1\n",
        "\n",
        "        reward_history.append(rewards)\n",
        "        total_rewards += rewards\n",
        "        average_reward_history.append(total_rewards / (cur_iter + 1))\n",
        "\n",
        "    return average_reward_history, reward_history, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0plpCks1WBXK",
        "outputId": "60ac0b49-5ebc-4261-9d3f-02bd31f5ac15"
      },
      "source": [
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Used: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlZWwrYuWHMg"
      },
      "source": [
        "env = gym.make(\"snake:snake-v0\")\n",
        "\n",
        "# Set up environment hyperparameters\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Set up training hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "tau = 1.0\n",
        "replace_iter = 10\n",
        "max_len = 1000\n",
        "iteration = 4000\n",
        "\n",
        "# Used for the exploration\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "\n",
        "# Build the model\n",
        "dqn_model = DQN(\n",
        "    num_actions=num_actions, \n",
        "    device=device, replace_iter=replace_iter,\n",
        "    max_len=max_len, EPS_START=EPS_START, EPS_END=EPS_END,\n",
        "    EPS_DECAY=EPS_DECAY\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7WOsNdkWb6N"
      },
      "source": [
        "# Set up optimizer and criterion\n",
        "optimizer = optim.Adam(dqn_model.eval_net.parameters(),\n",
        "                        lr=learning_rate)\n",
        "criterion = nn.SmoothL1Loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sTRKhL2XUsK",
        "outputId": "a18b6317-e073-42b2-8900-fd5f5b036c89"
      },
      "source": [
        "# Start training DQN\n",
        "average_reward_history, reward_history, \\\n",
        "    loss_history = train(\n",
        "        model=dqn_model, env=env, optimizer=optimizer,\n",
        "        criterion=criterion, iteration=iteration,\n",
        "        batch_size=batch_size, tau=tau, gamma=gamma\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========Iteration 1/4000============\n",
            "Iteration finished after 72 timesteps\n",
            "Get total rewards -6.420000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 2/4000============\n",
            "Iteration finished after 15 timesteps\n",
            "Get total rewards -5.28\n",
            "The length of the snake is 1\n",
            "===========Iteration 3/4000============\n",
            "Iteration finished after 33 timesteps\n",
            "Get total rewards -5.640000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 4/4000============\n",
            "Iteration finished after 49 timesteps\n",
            "Get total rewards -5.960000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 5/4000============\n",
            "Iteration finished after 16 timesteps\n",
            "Get total rewards -5.3\n",
            "The length of the snake is 1\n",
            "===========Iteration 6/4000============\n",
            "Iteration finished after 31 timesteps\n",
            "Get total rewards -5.6000000000000005\n",
            "The length of the snake is 1\n",
            "===========Iteration 7/4000============\n",
            "Iteration finished after 32 timesteps\n",
            "Get total rewards -5.62\n",
            "The length of the snake is 1\n",
            "===========Iteration 8/4000============\n",
            "Iteration finished after 15 timesteps\n",
            "Get total rewards -5.28\n",
            "The length of the snake is 1\n",
            "===========Iteration 9/4000============\n",
            "Iteration finished after 11 timesteps\n",
            "Get total rewards -5.2\n",
            "The length of the snake is 1\n",
            "===========Iteration 10/4000============\n",
            "Iteration finished after 37 timesteps\n",
            "Get total rewards -5.720000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 11/4000============\n",
            "Iteration finished after 23 timesteps\n",
            "Get total rewards -5.44\n",
            "The length of the snake is 1\n",
            "===========Iteration 12/4000============\n",
            "Iteration finished after 21 timesteps\n",
            "Get total rewards -5.4\n",
            "The length of the snake is 1\n",
            "===========Iteration 13/4000============\n",
            "Iteration finished after 12 timesteps\n",
            "Get total rewards -5.22\n",
            "The length of the snake is 1\n",
            "===========Iteration 14/4000============\n",
            "Iteration finished after 64 timesteps\n",
            "Get total rewards -6.260000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 15/4000============\n",
            "Iteration finished after 11 timesteps\n",
            "Get total rewards -5.2\n",
            "The length of the snake is 1\n",
            "===========Iteration 16/4000============\n",
            "Iteration finished after 19 timesteps\n",
            "Get total rewards -5.36\n",
            "The length of the snake is 1\n",
            "===========Iteration 17/4000============\n",
            "Iteration finished after 15 timesteps\n",
            "Get total rewards -5.28\n",
            "The length of the snake is 1\n",
            "===========Iteration 18/4000============\n",
            "Iteration finished after 11 timesteps\n",
            "Get total rewards -5.2\n",
            "The length of the snake is 1\n",
            "===========Iteration 19/4000============\n",
            "Iteration finished after 36 timesteps\n",
            "Get total rewards -5.7\n",
            "The length of the snake is 1\n",
            "===========Iteration 20/4000============\n",
            "Iteration finished after 42 timesteps\n",
            "Get total rewards -5.82\n",
            "The length of the snake is 1\n",
            "===========Iteration 21/4000============\n",
            "Iteration finished after 11 timesteps\n",
            "Get total rewards -5.2\n",
            "The length of the snake is 1\n",
            "===========Iteration 22/4000============\n",
            "Iteration finished after 50 timesteps\n",
            "Get total rewards -5.98\n",
            "The length of the snake is 1\n",
            "===========Iteration 23/4000============\n",
            "Iteration finished after 41 timesteps\n",
            "Get total rewards -5.800000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 24/4000============\n",
            "Iteration finished after 19 timesteps\n",
            "Get total rewards -5.36\n",
            "The length of the snake is 1\n",
            "===========Iteration 25/4000============\n",
            "Iteration finished after 120 timesteps\n",
            "Get total rewards -7.380000000000002\n",
            "The length of the snake is 1\n",
            "===========Iteration 26/4000============\n",
            "Iteration finished after 24 timesteps\n",
            "Get total rewards 14.560000000000002\n",
            "The length of the snake is 2\n",
            "===========Iteration 27/4000============\n",
            "Iteration finished after 19 timesteps\n",
            "Get total rewards -5.36\n",
            "The length of the snake is 1\n",
            "===========Iteration 28/4000============\n",
            "Iteration finished after 11 timesteps\n",
            "Get total rewards -5.2\n",
            "The length of the snake is 1\n",
            "===========Iteration 29/4000============\n",
            "Iteration finished after 26 timesteps\n",
            "Get total rewards -5.5\n",
            "The length of the snake is 1\n",
            "===========Iteration 30/4000============\n",
            "Iteration finished after 59 timesteps\n",
            "Get total rewards -6.16\n",
            "The length of the snake is 1\n",
            "===========Iteration 31/4000============\n",
            "Iteration finished after 64 timesteps\n",
            "Get total rewards -6.260000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 32/4000============\n",
            "Iteration finished after 66 timesteps\n",
            "Get total rewards -6.300000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 33/4000============\n",
            "Iteration finished after 66 timesteps\n",
            "Get total rewards -6.300000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 34/4000============\n",
            "Iteration finished after 32 timesteps\n",
            "Get total rewards -5.62\n",
            "The length of the snake is 1\n",
            "===========Iteration 35/4000============\n",
            "Iteration finished after 16 timesteps\n",
            "Get total rewards -5.3\n",
            "The length of the snake is 1\n",
            "===========Iteration 36/4000============\n",
            "Iteration finished after 157 timesteps\n",
            "Get total rewards -8.120000000000003\n",
            "The length of the snake is 1\n",
            "===========Iteration 37/4000============\n",
            "Iteration finished after 20 timesteps\n",
            "Get total rewards -5.38\n",
            "The length of the snake is 1\n",
            "===========Iteration 38/4000============\n",
            "Iteration finished after 83 timesteps\n",
            "Get total rewards -6.640000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 39/4000============\n",
            "Iteration finished after 432 timesteps\n",
            "Get total rewards -13.619999999999905\n",
            "The length of the snake is 1\n",
            "===========Iteration 40/4000============\n",
            "Iteration finished after 150 timesteps\n",
            "Get total rewards -7.980000000000002\n",
            "The length of the snake is 1\n",
            "===========Iteration 41/4000============\n",
            "Iteration finished after 21 timesteps\n",
            "Get total rewards -5.4\n",
            "The length of the snake is 1\n",
            "===========Iteration 42/4000============\n",
            "Iteration finished after 31 timesteps\n",
            "Get total rewards -5.6000000000000005\n",
            "The length of the snake is 1\n",
            "===========Iteration 43/4000============\n",
            "Iteration finished after 72 timesteps\n",
            "Get total rewards -6.420000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 44/4000============\n",
            "Iteration finished after 21 timesteps\n",
            "Get total rewards -5.4\n",
            "The length of the snake is 1\n",
            "===========Iteration 45/4000============\n",
            "Iteration finished after 20 timesteps\n",
            "Get total rewards 14.640000000000004\n",
            "The length of the snake is 2\n",
            "===========Iteration 46/4000============\n",
            "Iteration finished after 31 timesteps\n",
            "Get total rewards -5.6000000000000005\n",
            "The length of the snake is 1\n",
            "===========Iteration 47/4000============\n",
            "Iteration finished after 52 timesteps\n",
            "Get total rewards -6.0200000000000005\n",
            "The length of the snake is 1\n",
            "===========Iteration 48/4000============\n",
            "Iteration finished after 107 timesteps\n",
            "Get total rewards -7.120000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 49/4000============\n",
            "Iteration finished after 64 timesteps\n",
            "Get total rewards -6.260000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 50/4000============\n",
            "Iteration finished after 17 timesteps\n",
            "Get total rewards -5.32\n",
            "The length of the snake is 1\n",
            "===========Iteration 51/4000============\n",
            "Iteration finished after 574 timesteps\n",
            "Get total rewards -16.459999999999845\n",
            "The length of the snake is 1\n",
            "===========Iteration 52/4000============\n",
            "Iteration finished after 409 timesteps\n",
            "Get total rewards 6.860000000000085\n",
            "The length of the snake is 2\n",
            "===========Iteration 53/4000============\n",
            "Iteration finished after 92 timesteps\n",
            "Get total rewards -6.820000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 54/4000============\n",
            "Iteration finished after 37 timesteps\n",
            "Get total rewards -5.720000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 55/4000============\n",
            "Iteration finished after 31 timesteps\n",
            "Get total rewards -5.6000000000000005\n",
            "The length of the snake is 1\n",
            "===========Iteration 56/4000============\n",
            "Iteration finished after 93 timesteps\n",
            "Get total rewards -6.840000000000002\n",
            "The length of the snake is 1\n",
            "===========Iteration 57/4000============\n",
            "Iteration finished after 558 timesteps\n",
            "Get total rewards -16.13999999999985\n",
            "The length of the snake is 1\n",
            "===========Iteration 58/4000============\n",
            "Iteration finished after 220 timesteps\n",
            "Get total rewards -9.379999999999995\n",
            "The length of the snake is 1\n",
            "===========Iteration 59/4000============\n",
            "Iteration finished after 159 timesteps\n",
            "Get total rewards -8.160000000000002\n",
            "The length of the snake is 1\n",
            "===========Iteration 60/4000============\n",
            "Iteration finished after 604 timesteps\n",
            "Get total rewards -17.05999999999983\n",
            "The length of the snake is 1\n",
            "===========Iteration 61/4000============\n",
            "Iteration finished after 65 timesteps\n",
            "Get total rewards -6.280000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 62/4000============\n",
            "Iteration finished after 79 timesteps\n",
            "Get total rewards -6.5600000000000005\n",
            "The length of the snake is 1\n",
            "===========Iteration 63/4000============\n",
            "Iteration finished after 163 timesteps\n",
            "Get total rewards 31.799999999999834\n",
            "The length of the snake is 3\n",
            "===========Iteration 64/4000============\n",
            "Iteration finished after 483 timesteps\n",
            "Get total rewards -14.639999999999883\n",
            "The length of the snake is 1\n",
            "===========Iteration 65/4000============\n",
            "Iteration finished after 21 timesteps\n",
            "Get total rewards -5.4\n",
            "The length of the snake is 1\n",
            "===========Iteration 66/4000============\n",
            "Iteration finished after 22 timesteps\n",
            "Get total rewards -5.42\n",
            "The length of the snake is 1\n",
            "===========Iteration 67/4000============\n",
            "Iteration finished after 36 timesteps\n",
            "Get total rewards -5.7\n",
            "The length of the snake is 1\n",
            "===========Iteration 68/4000============\n",
            "Iteration finished after 203 timesteps\n",
            "Get total rewards -9.040000000000003\n",
            "The length of the snake is 1\n",
            "===========Iteration 69/4000============\n",
            "Iteration finished after 360 timesteps\n",
            "Get total rewards 7.840000000000064\n",
            "The length of the snake is 2\n",
            "===========Iteration 70/4000============\n",
            "Iteration finished after 39 timesteps\n",
            "Get total rewards 14.260000000000016\n",
            "The length of the snake is 2\n",
            "===========Iteration 71/4000============\n",
            "Iteration finished after 24 timesteps\n",
            "Get total rewards -5.46\n",
            "The length of the snake is 1\n",
            "===========Iteration 72/4000============\n",
            "Iteration finished after 131 timesteps\n",
            "Get total rewards -7.600000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 73/4000============\n",
            "Iteration finished after 106 timesteps\n",
            "Get total rewards -7.100000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 74/4000============\n",
            "Iteration finished after 156 timesteps\n",
            "Get total rewards -8.100000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 75/4000============\n",
            "Iteration finished after 451 timesteps\n",
            "Get total rewards -13.999999999999897\n",
            "The length of the snake is 1\n",
            "===========Iteration 76/4000============\n",
            "Iteration finished after 53 timesteps\n",
            "Get total rewards -6.040000000000001\n",
            "The length of the snake is 1\n",
            "===========Iteration 77/4000============\n",
            "Iteration finished after 24 timesteps\n",
            "Get total rewards 14.560000000000006\n",
            "The length of the snake is 2\n",
            "===========Iteration 78/4000============\n",
            "Iteration finished after 1000 timesteps\n",
            "Get total rewards -24.979999999999663\n",
            "The length of the snake is 1\n",
            "===========Iteration 79/4000============\n",
            "Iteration finished after 21 timesteps\n",
            "Get total rewards -5.4\n",
            "The length of the snake is 1\n",
            "===========Iteration 80/4000============\n",
            "Iteration finished after 14 timesteps\n",
            "Get total rewards -5.26\n",
            "The length of the snake is 1\n",
            "===========Iteration 81/4000============\n",
            "Iteration finished after 36 timesteps\n",
            "Get total rewards -5.7\n",
            "The length of the snake is 1\n",
            "===========Iteration 82/4000============\n",
            "Iteration finished after 162 timesteps\n",
            "Get total rewards -8.220000000000002\n",
            "The length of the snake is 1\n",
            "===========Iteration 83/4000============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ezv80qPXhT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}